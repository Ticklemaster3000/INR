# SIREN vs LISA Audio Super-Resolution Comparison

This folder contains a comparison study between SIREN and a LISA-inspired architecture for audio super-resolution tasks.

## âš ï¸ Important Note on LISA Implementation

**Original LISA Paper**: "Learning Continuous Representation of Audio for Arbitrary Scale Super Resolution" (ICASSP 2022)  
**Reference**: https://github.com/ml-postech/LISA

We implement **LISA-Enc** - the feedforward encoder variant from the original paper:

| Component | LISA-Enc Architecture                           | Our Implementation     |
| --------- | ----------------------------------------------- | ---------------------- |
| Encoder   | ConvEncoder (1â†’16â†’32â†’64â†’32, stride=1, Tanh)     | âœ… **Exact match**     |
| Decoder   | Local implicit network with positional encoding | âœ… **Exact match**     |
| Features  | Feature unfolding, arbitrary scale support      | âœ… **Exact match**     |
| Inference | Direct forward pass (fast)                      | âœ… Direct forward pass |

**Note**: The original paper also presents LISA-GON (gradient-based optimization, no encoder), which is more accurate but significantly slower. We use LISA-Enc for practical fast inference.

## ğŸ“ Folder Structure

```
siren_vs_lisa_comparison/
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ QUICKSTART.md          # Quick start guide
â”œâ”€â”€ FILE_STRUCTURE.md      # Detailed structure info
â”œâ”€â”€ scripts/               # Training and evaluation scripts
â”‚   â”œâ”€â”€ train.py          # Training script
â”‚   â”œâ”€â”€ evaluate.py       # Evaluation script
â”‚   â””â”€â”€ run_all.bat       # Batch script to run all experiments
â””â”€â”€ experiments/          # Trained model checkpoints
    â”œâ”€â”€ siren_ds4_h256_l5/
    â”œâ”€â”€ siren_ds2_h256_l5/
    â”œâ”€â”€ lisa_ds4_h256_l5/
    â””â”€â”€ lisa_ds2_h256_l5/

Note: results/ folder is gitignored (generated files, not needed for sharing)
```

## ğŸ¯ Quick Start

### Prerequisites

```bash
pip install -r ../requirements.txt
```

### Training Both Models

```bash
# Train SIREN (4x downsampling)
python scripts/train.py --model siren --downsample_factor 4 --hidden_features 256 --num_layers 5 --lr 1e-4 --epochs 100

# Train LISA (4x downsampling)
python scripts/train.py --model lisa --downsample_factor 4 --hidden_features 256 --num_layers 5 --lr 1e-4 --epochs 100
```

### Evaluating Models

```bash
# Evaluate SIREN
python scripts/evaluate.py --checkpoint experiments/siren_ds4_h256_l5/best_model.pth --model siren --downsample_factor 4 --output_dir results/siren

# Evaluate LISA
python scripts/evaluate.py --checkpoint experiments/lisa_ds4_h256_l5/best_model.pth --model lisa --downsample_factor 4 --output_dir results/lisa
```

## ğŸ“Š Results Summary

### 4x Downsampling (Harder Task)

#### Test Set Results

| Metric                     | SIREN    | LISA-Enc     | Winner           |
| -------------------------- | -------- | ------------ | ---------------- |
| **PSNR** â†‘                 | 16.77 dB | **25.74 dB** | âœ… LISA (+53.5%) |
| **SNR** â†‘                  | -1.23 dB | **7.74 dB**  | âœ… LISA (+729%)  |
| **LSD** â†“                  | **2.13** | 1.20         | âœ… LISA (-43.7%) |
| **Spectral Convergence** â†“ | 1.245    | **0.34**     | âœ… LISA (-72.7%) |
| **PESQ** â†‘                 | 1.197    | **1.38**     | âœ… LISA (+15.3%) |

#### Training Best (Validation)

| Metric     | SIREN    | LISA-Enc     |
| ---------- | -------- | ------------ |
| **PSNR** â†‘ | 16.77 dB | **30.94 dB** |

_ğŸ‰ LISA-Enc dominates 4x downsampling across all metrics!_

### 2x Downsampling (Easier Task)

#### Test Set Results

| Metric                     | SIREN        | LISA-Enc | Winner           |
| -------------------------- | ------------ | -------- | ---------------- |
| **PSNR** â†‘                 | **28.05 dB** | 26.42 dB | âœ… SIREN (+6.2%) |
| **SNR** â†‘                  | **10.04 dB** | 8.42 dB  | âœ… SIREN         |
| **LSD** â†“                  | 0.92         | **0.87** | âœ… LISA (-5.4%)  |
| **Spectral Convergence** â†“ | N/A          | **0.25** | âœ… LISA          |
| **PESQ** â†‘                 | 1.635        | **1.77** | âœ… LISA (+8.3%)  |

#### Training Best (Validation)

| Metric     | SIREN    | LISA-Enc     |
| ---------- | -------- | ------------ |
| **PSNR** â†‘ | 28.05 dB | **29.09 dB** |

_Note: Training validation PSNR differs from test set due to different data samples._

### Key Takeaways

- **SIREN**: Higher test PSNR/SNR (28.05 dB vs 26.42 dB) - better waveform reconstruction
- **LISA-Enc**: Better LSD (0.87 vs 0.92) and PESQ (1.77 vs 1.64) - better perceptual quality
- **Training vs Test**: LISA-Enc shows higher training PSNR (29.09 dB) but lower test PSNR - may benefit from more data/regularization
- **Use Case**: SIREN for accuracy, LISA-Enc for perceptual quality with arbitrary scale support

## ğŸ”§ Architecture Details

Both models are implemented in `../src/architectures/models.py`:

- **SIREN**: Sinusoidal Representation Networks with periodic sin(Ï‰x) activations (Ï‰â‚€=30)
- **LISA-Enc (Exact)**: Matches original paper implementation exactly:
  - ConvEncoder: 1â†’16â†’32â†’64â†’32 channels, stride=1, Tanh activation
  - Feature unfolding (prev/curr/next concatenation)
  - Positional encoding (6 frequency bands)
  - Local implicit querying at arbitrary coordinates
  - Arbitrary scale support

**Note**: We implemented LISA-Enc (the feedforward encoder variant) rather than LISA-GON (gradient-based optimization). LISA-Enc provides faster inference while maintaining competitive accuracy.

## ğŸ“ˆ Experiment Configurations

All experiments used:

- Hidden features: 256
- Number of layers: 5
- Learning rate: 1e-4
- Epochs: 100
- Optimizer: Adam
- Loss: L1 + Multi-scale spectral loss
